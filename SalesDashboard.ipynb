{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import date\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Database Connections\n",
    "# To publish the database on Looker Studio, open ngrok and enter the command: ngrok tcp 3306\n",
    "\n",
    "token = '?api_token=4ef0bf096ae0fdde361a40a120d18e1db1b973cc'\n",
    "pipedriveURL = 'https://snovio-5a30ce.pipedrive.com/api/v1/'\n",
    "\n",
    "def get_pipedrive(get, params=None):\n",
    "    url = f\"{pipedriveURL}{get}{token}\"  # Building the URL\n",
    "    response = requests.get(url, params=params)  # Using request\n",
    "    return response.json()\n",
    "\n",
    "connection = pymysql.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"root\",\n",
    "    password=\"shalazaki\",\n",
    "    database=\"pipedrive\"\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Preparing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_table(cursor, table_info):\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_info['name']} ({table_info['columns']})\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "tables_info = [\n",
    "    {\"name\": \"Deal_Stage\", \"columns\": \"id INT AUTO_INCREMENT, date_update DATETIME, deal_id INT, field_key VARCHAR(255), new_value VARCHAR(255), old_value VARCHAR(255), user_id INT, PRIMARY KEY (id, date_update, deal_id)\"},\n",
    "    {\"name\": \"User\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, name VARCHAR(255), timezone_name VARCHAR(255)\"},\n",
    "    {\"name\": \"Stage\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, order_nr VARCHAR(255), name VARCHAR(255), pipeline_id VARCHAR(255)\"},\n",
    "    {\"name\": \"Pipeline\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, name VARCHAR(255), order_nr VARCHAR(255)\"},\n",
    "    {\"name\": \"Lost_reason\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"Scheduled\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"Lead_Score_SDR\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"Lead_Score_AE\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"Team\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"Product\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"Contact_Made\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"Origin\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"Label\", \"columns\": \"id VARCHAR(255) PRIMARY KEY, label VARCHAR(255)\"},\n",
    "    {\"name\": \"max_update_time \", \"columns\": \"max_update_time  DATETIME\"}\n",
    "]\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    for table_info in tables_info:\n",
    "        create_table(cursor, table_info)\n",
    "\n",
    "# Make sure to commit to save the changes\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Collecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Collection Structure - Building Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### DEAL TABLE ############################################\n",
    "\n",
    "def create_table(cursor, table_name, columns):\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns}, page_number TEXT, PRIMARY KEY (id(255)))\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "\n",
    "def insert_data(connection, table, data_list, keys):\n",
    "    with connection.cursor() as cursor:\n",
    "        for data in data_list:\n",
    "            # Build ON DUPLICATE KEY UPDATE clause\n",
    "            update_columns = ', '.join([f\"{key} = %s\" for key in keys])\n",
    "            on_duplicate_key_update = f\" ON DUPLICATE KEY UPDATE {update_columns}\"\n",
    "\n",
    "            # Build the complete SQL query\n",
    "            sql = f\"INSERT INTO {table} ({', '.join(keys)}) VALUES ({', '.join(['%s'] * len(keys))}) ON DUPLICATE KEY UPDATE {update_columns}\"\n",
    "\n",
    "            # Adjust here: directly obtain values from the dictionary, handling 'page_number'\n",
    "            params = [data.get(key, '') if key != 'user_id' else data.get(key, '') for key in keys]  # Fixed here\n",
    "\n",
    "            cursor.execute(sql, params + params)  # Duplicate parameters for the ON DUPLICATE KEY UPDATE part\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "\n",
    "deal_keys = [\n",
    "    \"id\", \"title\", \"owner_name\", \"user_id\", \"value\", \"pipeline_id\", \"stage_id\",\n",
    "    \"add_time\", \"update_time\",\"won_time\",\"lost_time\" ,\"label\", \"status\", \"lost_reason\", \n",
    "    \"4399b1f494a290194a563ff8d051a2931004c32c\",  # Scheduled by\n",
    "    \"856481b27c6718cc6e2872f07d3b7c444114182a\",  # Lead Score SDR\n",
    "    \"8efa88d5da049834ad89cd5158238c463123d91d\",  # Lead Score AE\n",
    "    \"53835fe5eebad73f15ab9552c0cb30d6173565ce\",  # Team\n",
    "    \"58af6c9e65d4df2e0a1273a7bacfe8af10aac1e7\",  # Company Size\n",
    "    \"cd45c2d856d79a012fe5f723a2669058d5d9cb4c\",  # Product\n",
    "    \"021312270ecddc7d5167019c73b28ef262e04861\",  # Linkedin Profile\n",
    "    \"471938728cb3d88d9ccbcf9fe43799730b7bace8\",  # Country\n",
    "    \"39d1d775f22299ba35733d9a2f6ab73a69b7fb87\",  # Job Position\n",
    "    \"a4d67ee86fb981de06b4f0d63be05de236747474\",  # Main Goals\n",
    "    \"060949080c3a6c723d41c3eab8ee6389b9c6b9bf\",  # Contact Made\n",
    "    \"5d40211a6403fecbdc3c861191f66725ac1773b8\"  # Origin\n",
    "]\n",
    "\n",
    "# Create the table\n",
    "deal_columns = [f\"{key} TEXT\" for key in deal_keys]\n",
    "create_table(connection.cursor(), \"Deal\", ', '.join(deal_columns))\n",
    "\n",
    "# Get the last update timestamp\n",
    "cursor = connection.cursor()\n",
    "cursor.execute('SELECT MAX(update_time) FROM deal') # Getting the last deal update time from SQL\n",
    "timestamp_str = cursor.fetchone()[0] # The value will return as a string\n",
    "timestamp_pipedrive = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S') # Converting the string to date\n",
    "timestamp_subtracted = timestamp_pipedrive - timedelta(hours=4) # Subtracting 4 hours to align with Brazil time zone\n",
    "timestamp = timestamp_subtracted.strftime('%Y-%m-%d %H:%M:%S') # Converted to the last deal update time in Brazil time zone\n",
    "\n",
    "# timestamp = '2024-04-04 00:00:00' # USE THIS ONLY WHEN RESETTING THE DATABASE\n",
    "\n",
    "# Capturing data for the Deal table\n",
    "page_number = 0\n",
    "more_pages = True\n",
    "\n",
    "while more_pages:\n",
    "    get = 'recents'\n",
    "    params = {\"since_timestamp\":[timestamp],\"limit\":\"500\", \"start\":[page_number], \"items\":\"deal\"}\n",
    "    deal_data = get_pipedrive(get, params)\n",
    "\n",
    "    if deal_data['data'] is not None:\n",
    "        more_pages = deal_data['additional_data']['pagination']['more_items_in_collection']\n",
    "        \n",
    "        with connection.cursor() as cursor:\n",
    "            for deal_dictionary in deal_data['data']:\n",
    "                insert_data(connection, \"Deal\", [deal_dictionary['data']], deal_keys)\n",
    "                cursor.execute(f'UPDATE pipedrive.Deal SET page_number = %s WHERE page_number IS NULL', (page_number,))\n",
    "\n",
    "        page_number += 500\n",
    "    else:\n",
    "        more_pages = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Deals: 100%|██████████| 3112/3112 [55:40<00:00,  1.07s/it] \n"
     ]
    }
   ],
   "source": [
    "##################################### DEAL_STAGE ############################################\n",
    "\n",
    "# Get the last update timestamp\n",
    "cursor = connection.cursor()\n",
    "cursor.execute('SELECT MAX(date_update) FROM deal_stage') # Getting the last deal update time from SQL\n",
    "timestamp_subtracted = timestamp_pipedrive - timedelta(hours=4) # Subtracting 4 hours to align with Brazil time zone\n",
    "timestamp = timestamp_subtracted.strftime('%Y-%m-%d %H:%M:%S') # Converted to the last deal update time in Brazil time zone\n",
    "\n",
    "# Function to insert records into the Deal_Stage table\n",
    "def insert_deal_stage(params_list):\n",
    "    insert_sql = \"\"\"\n",
    "        INSERT INTO Deal_Stage (date_update, deal_id, field_key, new_value, old_value, user_id)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "        ON DUPLICATE KEY UPDATE\n",
    "        new_value = VALUES(new_value),\n",
    "        old_value = VALUES(old_value),\n",
    "        user_id = VALUES(user_id);\n",
    "    \"\"\"\n",
    "    with connection.cursor() as cursor:\n",
    "        for params in params_list:\n",
    "            # Check if the record already exists in the table\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT COUNT(*)\n",
    "                FROM Deal_Stage\n",
    "                WHERE date_update = %s\n",
    "                AND deal_id = %s\n",
    "                AND field_key = %s;\n",
    "            \"\"\", (params[0], params[1], params[2]))\n",
    "            count = cursor.fetchone()[0]\n",
    "\n",
    "            if count == 0:\n",
    "                cursor.execute(insert_sql, params)\n",
    "                connection.commit()\n",
    "\n",
    "# Capturing Deal IDs\n",
    "# Resume variable for update_time >= {timestamp}\n",
    "def get_deal_ids():\n",
    "    select_deal_ids_sql = f\"\"\"\n",
    "        SELECT id FROM deal\n",
    "        WHERE update_time >= \"{timestamp}\";\n",
    "    \"\"\"\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(select_deal_ids_sql)\n",
    "        return [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "# Capturing deal flow data\n",
    "def get_deal_flow(deal_id):\n",
    "    get = f\"deals/{deal_id}/flow\"\n",
    "    params = {\"items\":\"dealChange\",\"limit\":\"500\"}\n",
    "    return get_pipedrive(get,params)\n",
    "\n",
    "\n",
    "def main():\n",
    "    batch_size = 100  # Batch size for processing\n",
    "\n",
    "    deal_ids_list = get_deal_ids()\n",
    "    total_deals = len(deal_ids_list)  # Get the total number of deals\n",
    "    batches = [deal_ids_list[i:i+batch_size] for i in range(0, len(deal_ids_list), batch_size)]\n",
    "\n",
    "    processed_deals = 0  # Initialize the counter for processed deals\n",
    "\n",
    "    with tqdm(total=total_deals, desc=\"Processing Deals\", disable=False) as pbar:\n",
    "        for batch in batches:\n",
    "            params_list = []\n",
    "\n",
    "            for deal_id in batch:\n",
    "                dealflow = get_deal_flow(deal_id)\n",
    "                data_list = dealflow.get('data', [])\n",
    "                \n",
    "                if data_list is not None:\n",
    "                    # If data is available, proceed with processing\n",
    "                    filtered_results = [\n",
    "                        item for item in data_list \n",
    "                        if item.get('object') == 'dealChange' and \n",
    "                        (item.get('data', {}).get('field_key') == 'stage_id' or \n",
    "                         item.get('data', {}).get('field_key') == 'status' or \n",
    "                         item.get('data', {}).get('field_key') == 'user_id' or\n",
    "                         item.get('data', {}).get('field_key') == 'add_time')\n",
    "                    ]\n",
    "                    for result in filtered_results:\n",
    "                        date_update = result['timestamp']\n",
    "                        field_key = result['data']['field_key']\n",
    "                        new_value = result['data']['new_value']\n",
    "                        old_value = result['data']['old_value']\n",
    "                        user_id = result['data']['user_id']\n",
    "                        params_list.append((date_update, deal_id, field_key, new_value, old_value, user_id))\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            insert_deal_stage(params_list)\n",
    "            processed_deals += len(batch)  # Update the counter for processed deals\n",
    "\n",
    "            pbar.update(len(batch))  # Update the progress bar\n",
    "\n",
    "    return \"All deals processed.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Structure - Enriching Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate User and Scheduled Tables to receive new information\n",
    "New_Values_UsersAndScheduled = [\n",
    "    \"\"\"\n",
    "    TRUNCATE TABLE user;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    TRUNCATE TABLE scheduled;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    TRUNCATE TABLE max_update_time;\n",
    "    \"\"\",\n",
    "\n",
    "]\n",
    "\n",
    "# Execute the SQL commands\n",
    "for command in New_Values_UsersAndScheduled:\n",
    "    cursor.execute(command)\n",
    "\n",
    "# Commit the changes\n",
    "connection.commit()\n",
    "\n",
    "\n",
    "#======================================================================\n",
    "#====== 1 - NATIVE PARAMETERS:\n",
    "#======================================================================  \n",
    "\n",
    "\n",
    "def insert_data(connection, table, data_list, keys):\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    for data in data_list:\n",
    "        columns = ', '.join(keys)\n",
    "        values = ', '.join([f\"'{data.get(key, '')}'\" for key in keys])\n",
    "        sql = f\"INSERT INTO {table} ({columns}) VALUES ({values}) ON DUPLICATE KEY UPDATE id=id\"\n",
    "        cursor.execute(sql)\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "data_mappings = {\n",
    "    \"users\": {\"table\": \"User\", \"keys\": [\"id\", \"name\", \"timezone_name\"]},\n",
    "    \"stages\": {\"table\": \"Stage\", \"keys\": [\"id\", \"order_nr\", \"name\", \"pipeline_id\"]},\n",
    "    \"pipelines\": {\"table\": \"Pipeline\", \"keys\": [\"id\", \"order_nr\", \"name\"]}\n",
    "}\n",
    "\n",
    "# Add code for other tables in the same loop\n",
    "for get, mapping in data_mappings.items():\n",
    "    data_list = get_pipedrive(get)['data']\n",
    "    keys = mapping[\"keys\"]\n",
    "    insert_data(connection, mapping[\"table\"], data_list, keys)\n",
    "\n",
    "\n",
    "\n",
    "#======================================================================\n",
    "#====== 2 - CUSTOM PARAMETERS:\n",
    "#======================================================================  \n",
    "\n",
    "\n",
    "# Parameter and table dictionaries\n",
    "parameter_table_mapping = {\n",
    "    '4399b1f494a290194a563ff8d051a2931004c32c': 'scheduled',\n",
    "    '856481b27c6718cc6e2872f07d3b7c444114182a': 'lead_score_sdr',\n",
    "    \"8efa88d5da049834ad89cd5158238c463123d91d\": 'lead_score_ae',\n",
    "    '53835fe5eebad73f15ab9552c0cb30d6173565ce': 'team',\n",
    "    'cd45c2d856d79a012fe5f723a2669058d5d9cb4c': 'product',\n",
    "    '060949080c3a6c723d41c3eab8ee6389b9c6b9bf': 'contact_made',\n",
    "    '5d40211a6403fecbdc3c861191f66725ac1773b8': 'origin',\n",
    "    'label':'label',\n",
    "    \"lost_reason\":\"lost_reason\"\n",
    "}\n",
    "\n",
    "table_columns_mapping = {\n",
    "    'scheduled': ('id', 'label'),\n",
    "    'lead_score_sdr': ('id', 'label'),\n",
    "    'lead_score_ae': ('id', 'label'),\n",
    "    'team': ('id', 'label'),\n",
    "    'product': ('id', 'label'),\n",
    "    'contact_made': ('id', 'label'),\n",
    "    'origin': ('id', 'label'),\n",
    "    'label': ('id','label'),\n",
    "    'lost_reason': ('id','label')\n",
    "}\n",
    "\n",
    "get = 'dealFields'\n",
    "deal_data = get_pipedrive(get)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "for parameter, table_name in parameter_table_mapping.items():\n",
    "    new_dict = None\n",
    "    for field_dict in deal_data['data']:\n",
    "        if 'key' in field_dict and field_dict['key'] == parameter:\n",
    "            new_dict = field_dict\n",
    "            break\n",
    "    \n",
    "    if new_dict is not None and 'options' in new_dict:\n",
    "        for option in new_dict['options']:\n",
    "            insert_columns = ', '.join(table_columns_mapping[table_name])\n",
    "            insert_values = ', '.join([f\"'{option[column]}'\" for column in table_columns_mapping[table_name]])\n",
    "            insert_query = f\"INSERT INTO {table_name} ({insert_columns}) VALUES ({insert_values}) ON DUPLICATE KEY UPDATE id=id\"\n",
    "            cursor.execute(insert_query)\n",
    "\n",
    "# Make sure to commit to save the changes\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== 1 - DEAL_FILTERED TABLE ========================================\n",
    "\n",
    "sql_commands = [\n",
    "    \"DROP TABLE IF EXISTS deal_filtered;\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE deal_filtered AS\n",
    "    SELECT * FROM deal;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE deal_filtered\n",
    "    CHANGE COLUMN `4399b1f494a290194a563ff8d051a2931004c32c` scheduled_by TEXT,\n",
    "    CHANGE COLUMN `856481b27c6718cc6e2872f07d3b7c444114182a` lead_score_sdr TEXT,\n",
    "    CHANGE COLUMN `8efa88d5da049834ad89cd5158238c463123d91d` lead_score_ae TEXT,\n",
    "    CHANGE COLUMN `53835fe5eebad73f15ab9552c0cb30d6173565ce` team TEXT,\n",
    "    CHANGE COLUMN `58af6c9e65d4df2e0a1273a7bacfe8af10aac1e7` company_size TEXT,\n",
    "    CHANGE COLUMN `cd45c2d856d79a012fe5f723a2669058d5d9cb4c` product TEXT,\n",
    "    CHANGE COLUMN `021312270ecddc7d5167019c73b28ef262e04861` linkedin_profile TEXT,\n",
    "    CHANGE COLUMN `471938728cb3d88d9ccbcf9fe43799730b7bace8` country TEXT,\n",
    "    CHANGE COLUMN `39d1d775f22299ba35733d9a2f6ab73a69b7fb87` job_position TEXT,\n",
    "    CHANGE COLUMN `a4d67ee86fb981de06b4f0d63be05de236747474` main_goals TEXT,\n",
    "    CHANGE COLUMN `060949080c3a6c723d41c3eab8ee6389b9c6b9bf` contact_made TEXT,\n",
    "    CHANGE COLUMN `5d40211a6403fecbdc3c861191f66725ac1773b8` origin TEXT;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    UPDATE deal_filtered\n",
    "    SET \n",
    "    add_time = DATE_SUB(add_time, INTERVAL 3 HOUR),\n",
    "    update_time = DATE_SUB(update_time, INTERVAL 3 HOUR),\n",
    "    lost_time = DATE_SUB(lost_time, INTERVAL 3 HOUR);\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    ALTER TABLE deal_filtered\n",
    "    ADD COLUMN Deal_Label VARCHAR(255);\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "# Execute the SQL commands\n",
    "for command in sql_commands:\n",
    "    cursor.execute(command)\n",
    "\n",
    "# Commit the changes\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add later\n",
    "#\"ALTER TABLE user ADD COLUMN SDR_ID INT;\",\n",
    "#\"UPDATE user u JOIN scheduled s ON u.name LIKE CONCAT('%', s.label, '%') SET u.SDR_ID = s.id;\",\n",
    "#\"UPDATE user SET SDR_ID = 275 WHERE name = 'Volodymyr';\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== 2 - New_deal_Stage Table\n",
    "\n",
    "# SQL commands\n",
    "sql_commands = [\n",
    "    \"DROP TABLE IF EXISTS new_deal_stage;\",\n",
    "    \"CREATE TABLE new_deal_stage (date_updated DATETIME, deal_id INT, new_value INT, old_value INT);\",\n",
    "    \"INSERT INTO new_deal_stage SELECT date_update, deal_id, new_value, old_value FROM deal_stage WHERE field_key = 'stage_id';\",\n",
    "    \"INSERT INTO new_deal_stage (date_updated, deal_id, new_value, old_value) SELECT NULL, deal_id, old_value AS new_value, old_value FROM (SELECT deal_id, old_value FROM new_deal_stage WHERE (deal_id, date_updated) IN (SELECT deal_id, MIN(date_updated) FROM new_deal_stage GROUP BY deal_id)) AS latest_deals;\",\n",
    "    \"ALTER TABLE new_deal_stage DROP COLUMN old_value;\",\n",
    "    \"UPDATE new_deal_stage SET date_updated = DATE_SUB(date_updated, INTERVAL 3 HOUR);\",\n",
    "    \"UPDATE new_deal_stage h1 JOIN (SELECT id, add_time FROM deal GROUP BY id) h2 ON h1.deal_id = h2.id SET h1.date_updated = h2.add_time WHERE h1.date_updated IS NULL;\",\n",
    "    \"INSERT INTO new_deal_stage (deal_id, date_updated, new_value) SELECT d.id, d.add_time, d.stage_id FROM deal d LEFT JOIN new_deal_stage nds ON d.id = nds.deal_id WHERE nds.deal_id IS NULL;\",\n",
    "\n",
    "]\n",
    "\n",
    "# Execute the SQL commands\n",
    "for command in sql_commands:\n",
    "    cursor.execute(command)\n",
    "\n",
    "# Commit the changes\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== 3 - Adapting user and deal_filtered ==================\n",
    "\n",
    "# SQL commands\n",
    "sql_commands = [\n",
    "    \"\"\"UPDATE user u\n",
    "    JOIN scheduled s ON u.name LIKE CONCAT('%', s.label, '%')\n",
    "    SET u.SDR_ID = s.id;\"\"\",\n",
    "    \n",
    "    \"\"\"UPDATE user\n",
    "    SET SDR_ID = 275\n",
    "    WHERE name = 'Volodymyr';\"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    update user\n",
    "    set SDR_ID = 329\n",
    "    where name = 'Nathalia Leichsnering Oliveira';\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    update user\n",
    "    set SDR_ID = 294\n",
    "    where name = 'Sofiia';\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"UPDATE user u\n",
    "    SET u.SDR_ID = u.id\n",
    "    WHERE u.SDR_ID IS NULL;\"\"\",\n",
    "\n",
    "    \"\"\"DELETE FROM user\n",
    "    WHERE id = '15505148';\"\"\",\n",
    "\n",
    "    \"\"\" \n",
    "    INSERT INTO max_update_time (max_update_time)\n",
    "    SELECT MAX(update_time) AS max_update_time\n",
    "    FROM deal_filtered;\n",
    "\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    UPDATE deal_filtered\n",
    "    SET Deal_Label = (\n",
    "        CASE\n",
    "            WHEN label = '65' THEN 'upsell'\n",
    "            WHEN label = '286' THEN 'personal email'\n",
    "            WHEN label = '287' THEN 'outbound'\n",
    "            ELSE 'inbound'\n",
    "        END\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute the SQL commands\n",
    "for command in sql_commands:\n",
    "    cursor.execute(command)\n",
    "\n",
    "# Commit the changes\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
    "language_info": {
      "name": "python",
      "version": "3.12.2",
      "mimetype": "text/x-python",
      "file_extension": ".py"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
